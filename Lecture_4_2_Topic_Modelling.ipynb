{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Операции с матрицами\n",
    "\n",
    "Матрицы *A* и *B* могут быть перемножены, если они совместимы в том смысле, что число столбцов матрицы *A* равно числу строк *B*. \n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\left(\n",
    "\\begin{array}{cccc}\n",
    "a_{11} & a_{12} & \\ldots & a_{1n}\\\\\n",
    "a_{21} & a_{22} & \\ldots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n1} & a_{n2} & \\ldots & a_{nn}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "B = \\left(\n",
    "\\begin{array}{cccc}\n",
    "b_{11} & b_{12} & \\ldots & b_{1n}\\\\\n",
    "b_{21} & b_{22} & \\ldots & b_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "b_{n1} & b_{n2} & \\ldots & b_{nn}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Тогда существует матрица *C*, такая что $c_{ij}=\\sum_{r=1}^m{a_{ir}b_{rj}}$\n",
    "\n",
    "$\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "1 & 2 & 3\\\\\n",
    "0 & 1 & 2\\\\\n",
    "2 & 3 & 4\n",
    "\\end{array}\n",
    "\\right)\n",
    "*\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "1 & 2 & 3\\\\\n",
    "0 & 1 & 2\\\\\n",
    "2 & 3 & 4\n",
    "\\end{array}\n",
    "\\right)\n",
    "= \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "1*1+2*0+3*2 & 1*2+2*1+3*3 & 1*3+2*2+3*4\\\\\n",
    "0*1+1*0+2*2 & 0*2+1*1+2*3 & 0*3+1*2+2*4\\\\\n",
    "2*1+3*0+4*2 & 2*2+3*1+4*3 & 2*3+3*2+4*4\n",
    "\\end{array}\n",
    "\\right)\n",
    "= \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "7 & 13 & 19\\\\\n",
    "4 & 7 & 10\\\\\n",
    "10 & 19 & 28\n",
    "\\end{array}\n",
    "\\right)\n",
    "$\n",
    "\n",
    "Для матрицы *А* может существовать обратная матрица $A^*$ такая, что $A*A^*=I$, где $I$ - единичная матрица, у которой все элементы, кроме диагональных, равны 0, а диагональные равны 1.\n",
    "\n",
    "Мы можем считать, что матрица состоит из векторов, то есть описывает совокупность точек. Тогда умножение такой матрицы на диагональную матрицу с ненулевыми элементами на диагонали эквивалентно масштабированию векторов по этим координатам.\n",
    "\n",
    "$\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "1 & 2 & 3\\\\\n",
    "0 & 1 & 2\\\\\n",
    "2 & 3 & 4\n",
    "\\end{array}\n",
    "\\right)\n",
    "*\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "2 & 0 & 0\\\\\n",
    "0 & 3 & 0\\\\\n",
    "0 & 0 & 4\n",
    "\\end{array}\n",
    "\\right)\n",
    "= \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "2 & 6 & 12\\\\\n",
    "0 & 3 & 8\\\\\n",
    "4 & 9 & 16\n",
    "\\end{array}\n",
    "\\right)\n",
    "$\n",
    "\n",
    "Матрицей поворота в n-мерном пространстве в некоторой плоскости на угол $\\alpha$ будет матрица следующего вида.\n",
    "\n",
    "\\begin{equation*}\n",
    "B = \\left(\n",
    "\\begin{array}{ccccc}\n",
    "\\cos{\\alpha} & -\\sin{\\alpha} & 0 & \\ldots & 0\\\\\n",
    "\\sin{\\alpha} & \\cos{\\alpha} & 0 &\\ldots & 0\\\\\n",
    "0 & 0 & 1 &\\ldots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & 0 & \\ldots & 1\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Здесь косинусы и синусы угла $\\alpha$ должны стоять на пересечении тех строк и столбцов, которые представляют две координаты плоскости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо этого, существует разложение матрицы на произведение двух матриц, называемое факторизацией. Пусть дана матрица $A$, тогда при соблюдении некоторых условий должны существовать матрицы $B$ и $C$ такие, что $A=BC$.\n",
    "\n",
    "Например, при решении системы линейных алгебраичесских уравнений используются методы LU, LQ, QR и других разложений, которые позволяют решить систему гораздо быстрее. В их основе лежит представление матрицы в виде произведения двух матриц.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  2,  3,  4],\n",
       "        [11, 12, 13, 14],\n",
       "        [ 1,  2,  3,  4],\n",
       "        [11, 12, 13, 14]]),\n",
       " array([2, 4, 6, 8]),\n",
       " array([ 60, 260,  60, 260]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4], [11,12,13,14], [1,2,3,4], [11,12,13,14]])\n",
    "x = np.array([2,4,6,8])\n",
    "b = np.dot(a, x)\n",
    "a, x, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.06401844,  0.70420284,  0.70691342,  0.01653513],\n",
       "        [-0.70420284, -0.06401844,  0.01653513, -0.70691342],\n",
       "        [-0.06401844,  0.70420284, -0.70691342, -0.01653513],\n",
       "        [-0.70420284, -0.06401844, -0.01653513,  0.70691342]]),\n",
       " array([[-1.56204994e+01, -1.71569419e+01, -1.86933845e+01,\n",
       "         -2.02298270e+01],\n",
       "        [ 0.00000000e+00,  1.28036880e+00,  2.56073760e+00,\n",
       "          3.84110640e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  4.95309802e-15,\n",
       "          4.84379964e-15],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         -3.02824409e-16]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, r = np.linalg.qr(a)\n",
    "q, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.],\n",
       "       [11., 12., 13., 14.],\n",
       "       [ 1.,  2.,  3.,  4.],\n",
       "       [11., 12., 13., 14.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(q, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ QRx = b$  \n",
    "$ Q^TQRx = Q^Tb$  \n",
    "$ Rx = Q^Tb$  \n",
    "$ R^{-1}Rx = R^{-1}Q^Tb$  \n",
    "$ x = R^{-1}Q^Tb$  \n",
    "\n",
    "Здесь следует обратить внимание, что матрица $R$ является верхнетреугольной, поэтому её обратная матрица ищется значительно проще. Как следствие, решение уравнений ведется быстрее. Самое главное, что если нам надо решать одну и ту же систему уравнений для разных входных значений, то перемножить матрицы можно один раз заранее.\n",
    "\n",
    "На практитке выяснятеся, что обычная инверсия работает не совсем корректно на наших данных, поэтому здесь я использую дооооолгий метод, включающий SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.56204994e+01, -1.71569419e+01, -1.86933845e+01,\n",
       "         -2.02298270e+01],\n",
       "        [ 0.00000000e+00,  1.28036880e+00,  2.56073760e+00,\n",
       "          3.84110640e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  4.95309802e-15,\n",
       "          4.84379964e-15],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         -3.02824409e-16]]),\n",
       " array([[-6.40184400e-02, -8.57847096e-01,  2.01893844e+14,\n",
       "         -3.37511324e+15],\n",
       "        [ 0.00000000e+00,  7.81024968e-01, -4.03787688e+14,\n",
       "          3.44798276e+15],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  2.01893844e+14,\n",
       "          3.22937419e+15],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -3.30224371e+15]]),\n",
       " array([[ 1.00000000e+00, -1.66694210e-16,  3.03095384e-17,\n",
       "         -3.87726925e-18],\n",
       "        [ 7.70754001e-18,  1.00000000e+00,  1.55815931e-15,\n",
       "         -7.56067503e-17],\n",
       "        [ 3.03095384e-17,  1.55815931e-15,  2.42877909e-30,\n",
       "         -1.17924880e-31],\n",
       "        [-3.87726925e-18, -7.56067503e-17, -1.17924880e-31,\n",
       "          5.73141391e-33]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, np.linalg.inv(r), np.dot(r, np.linalg.pinv(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 4., 6., 8.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.dot(np.linalg.pinv(r), q.transpose()), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тематическое моделирование\n",
    "\n",
    "Мы уже использовали снижение размерности пространства при анализе текстов. Еще одним вариантом такого снижения является тематическое моделирование. Для него используются следующие рассуждения.\n",
    "\n",
    "Пусть дан набор текстов, в каждом тексте имеются определенные слова. В таком случае можно посчитать матрицу термины-документы, в которой по строкам будут идти термины, по столбцам документы, а на пересечении будет стоять частота термина в данном документе. \n",
    "<img  width=\"40%\" src=\"img/term-document-matrix-bow-annotated.png\">\n",
    "\n",
    "\n",
    "При помощи элементарных математических преобразований (метод SVD) данную матрицу можно представить в виде произведения трех матриц: матрицы слово на тематику, матрицы тематик и матрицы тематика на документ. Средняя из матриц будет квадратной и диагональной и будет содержать некоторые коэффициенты важности, которые можно отсортировать по убыванию. В этом случае размерность матрицы можно серьезно сократить, отбросив неактуальные темы. Для сокращения числа коэффициентов надо снизить размер матрицы $\\Sigma$.\n",
    "\n",
    "<img  width=\"40%\" src=\"img/1024px-Singular-Value-Decomposition.svg.png\"> \n",
    "<img  width=\"40%\" src=\"img/800px-Singular_value_decomposition_visualisation.svg.png\">\n",
    "\n",
    " $U U^∗ = I$ и $V V^∗ = I$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе SVD работает метод латентно-семантического анализа (LSA), который позволяет более четко найти связи между терминами и документами. Его более быстрая реализация, LDA, проводит нечеткий поиск. LSI является модификацией для задач информационного поиска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим теперь пример применения SVD для задач классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pymorphy2\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import umap\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Разложение матрицы на три с сокращением размерности.\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Разложение матрицы на произведение двух.\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем по тысяче научных текстов из пяти разных областей науки. (Они занимают 140 Мб и не влезают на Гит, так что доступны по запросу.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_texts = pd.read_csv(\"data/kaggle-science-texts_train.tsv\", header=0, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Target                                               Text\n",
       "0   1       0                                                ...\n",
       "1   2       0                                                ...\n",
       "2   3       0                                                ...\n",
       "3   4       0                                                ...\n",
       "4   5       0                                                ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sci_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 8%\n",
      "(5000, 20) [[ 0.18716104  0.05641818 -0.0960269  ... -0.05742942 -0.09035204\n",
      "  -0.00729402]\n",
      " [ 0.15355927 -0.0017868  -0.0600151  ...  0.01290683 -0.02614199\n",
      "   0.00550857]\n",
      " [ 0.12449795  0.01570744 -0.06641631 ... -0.09204764  0.02928297\n",
      "  -0.0480637 ]\n",
      " ...\n",
      " [ 0.27208788 -0.08811325 -0.06229885 ... -0.1048896  -0.06448673\n",
      "  -0.03261453]\n",
      " [ 0.16188171 -0.07406504 -0.04232777 ... -0.05924854 -0.02868713\n",
      "  -0.02078627]\n",
      " [ 0.33845172 -0.15522904 -0.07595739 ... -0.03040848 -0.1027167\n",
      "  -0.00082302]]\n",
      "CPU times: user 17.4 s, sys: 10.5 s, total: 27.9 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Считаем матрицу термин-документ, но не с частотами, а со значениями Tf*Idf\n",
    "X = TfidfVectorizer().fit_transform(sci_texts['Text'])\n",
    "# Проводим SVD-разложение по 20 компонентам.\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "X2 = svd.fit_transform(X)\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(f\"Explained variance of the SVD step: {int(explained_variance * 100)}%\")\n",
    "# Просто чтобы посмотреть, что там в самом деле вектора.\n",
    "print(X2.shape, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x509412 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5699630 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходно, код ниже выполнялся 37 минут. Но если поставить  \n",
    "`pip install pymorphy2[fast]`\n",
    "то библиотека начинает работать горазо быстрее.\n",
    "\n",
    "Если морфология работает медленно, но в тексте вссегда есть повторяющиеся слова (кто знает что такое закон Ципфа?), то может быть результаты медленной морфологии надо кешировать в быстром словаре (dict)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список значимых частей речи. \n",
    "# Они нам потом понадобятся в немного другом виде. Так что сделаем словарь. чтобы два раза не вставать.\n",
    "conv_pos = {'ADJF':'ADJ', 'ADJS':'ADJ', 'ADV':'ADV', 'NOUN':'NOUN', \n",
    "            'VERB':'VERB', 'PRTF':'ADJ', 'PRTS':'ADJ', 'GRND':'VERB'}\n",
    "\n",
    "tmp_dict = {} # Кеш значимых слов.\n",
    "nones = {} # Кеш незначимых слов.\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Фильтруем по части речи и возвращаем только начальную форму.\n",
    "def normalizePymorphy(text, need_pos=True):\n",
    "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        # Если токен уже был закеширован, быстро возьмем результат из него.\n",
    "        if t in tmp_dict.keys():\n",
    "            words.append(tmp_dict[t])\n",
    "        # Аналогично, если он в кеше незначимых слов.\n",
    "        elif t in nones.keys():\n",
    "            pass\n",
    "        # Слово еще не встретилось, будем проводить медленный морфологический анализ.\n",
    "        else:\n",
    "            pv = morph.parse(t)\n",
    "            if pv[0].tag.POS != None:\n",
    "                if pv[0].tag.POS in conv_pos.keys():\n",
    "                    if need_pos:\n",
    "                        word = pv[0].normal_form+\"_\"+conv_pos[pv[0].tag.POS]\n",
    "                    else:\n",
    "                        word = pv[0].normal_form\n",
    "                    # Отправляем слово в результат, ...\n",
    "                    words.append(word)\n",
    "                    # ... и кешируем результат его разбора.\n",
    "                    tmp_dict[t] = word\n",
    "                else:\n",
    "                    # Для незначимых слов можно даже ничего не хранить. Лишь бы потом не обращаться к морфологии.\n",
    "                    nones[t] = \"\"\n",
    "                    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 64.6 ms, total: 1min 24s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Приведем слова в текстах к начальным формам.\n",
    "sci_texts['NText'] = sci_texts['Text'].map(lambda x:' '.join(normalizePymorphy(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 7.93 s, total: 19.8 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_l = TfidfVectorizer().fit_transform(sci_texts['NText'])\n",
    "svd = TruncatedSVD(n_components=10)\n",
    "X2_l = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим целевые переменные. Мы же знаем, что там ровно по 1000 текстов для каждой области.\n",
    "classes = np.ones(5000)\n",
    "for i in range(2, 6):\n",
    "    classes[(i-1)*1000: i*1000] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_texts(data, target):\n",
    "    # Делим данные на обучающую и проверочную выборки.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=333)\n",
    "    #Обучаем классификатор и оцениваем точность результатов.\n",
    "    tree = RandomForestClassifier(criterion='entropy', random_state=333)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_hat = tree.predict(X_test)\n",
    "    print(f\"accuracy = {accuracy_score(y_hat, y_test)}\")\n",
    "    print(confusion_matrix(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь сравним точность работы классификатора, запущенного на разных данных: с и без лемматизации; векторизация при помощи TF*IDF или SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.979\n",
      "[[204   0   3   0   2]\n",
      " [  2 185   2   1   0]\n",
      " [  4   0 199   2   1]\n",
      " [  0   0   1 190   2]\n",
      " [  0   0   1   0 201]]\n",
      "CPU times: user 14.8 s, sys: 23.8 ms, total: 14.8 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF*Idf без лемматизации.\n",
    "classify_texts(X, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.97\n",
      "[[201   0   3   2   3]\n",
      " [  4 180   6   0   0]\n",
      " [  3   0 200   2   1]\n",
      " [  1   0   0 190   2]\n",
      " [  0   0   0   3 199]]\n",
      "CPU times: user 8.15 s, sys: 7.51 ms, total: 8.16 s\n",
      "Wall time: 8.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF*Idf с лемматизацией.\n",
    "classify_texts(X_l, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.971\n",
      "[[202   1   4   1   1]\n",
      " [  2 181   6   1   0]\n",
      " [  1   1 202   2   0]\n",
      " [  0   2   2 187   2]\n",
      " [  0   0   0   3 199]]\n",
      "CPU times: user 1.3 s, sys: 3.45 ms, total: 1.31 s\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVD без лемматизации.\n",
    "classify_texts(X2, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.967\n",
      "[[203   1   3   1   1]\n",
      " [  2 179   9   0   0]\n",
      " [  3   1 200   2   0]\n",
      " [  1   2   1 187   2]\n",
      " [  0   0   0   4 198]]\n",
      "CPU times: user 894 ms, sys: 275 µs, total: 895 ms\n",
      "Wall time: 894 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVD с лемматизацией.\n",
    "classify_texts(X2_l, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что с 10 компонентами, полученными при помощи SVD работает в 10 раз быстрее и немного точнее.\n",
    "\n",
    "Вообще, для тематического моделирования текстов существует большая библиотека [BigARTM](http://bigartm.org/), но она не ставится так просто и не входит в стандартные библиотеки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё можно разложить на две матрицы. Это будет аналог кластеризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 4.53 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nm = NMF(n_components=10)\n",
    "X_nm = nm.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
